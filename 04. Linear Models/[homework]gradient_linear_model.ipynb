{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[hw_students]gradient_linear_model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"wCLhRmJbQTFc"},"cell_type":"markdown","source":["<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=300, height=300></p>\n","\n","<h3 style=\"text-align: center;\"><b>Phystech School of Applied Mathematics and Informatics (PAMI) MIPT</b></h3>"]},{"metadata":{"colab_type":"text","id":"gTolUBb8QTFg"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"D4UidgYLQTFj"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Gradient descent and linear models: Homework</b></h3>"]},{"metadata":{"colab_type":"code","id":"Kk37UfUTQTFo","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy\n","from matplotlib import pylab, gridspec, pyplot as plt\n","\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"c_g7Oj5dQTF_"},"cell_type":"markdown","source":["### 1. Gradient descent. Reiteration"]},{"metadata":{"colab_type":"text","id":"6q2oYX8hQTGD"},"cell_type":"markdown","source":["Let's have a look to a fucntion with two arguments:"]},{"metadata":{"colab_type":"code","id":"dAsqqdZXQTGG","colab":{}},"cell_type":"code","source":["def f(x1, x2):\n","    return np.sin(x1)**2 + np.sin(x2) ** 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RJqCZURx1SOm","colab_type":"text"},"cell_type":"markdown","source":["**Reminder**:<br>\n","What do we want? We want to find the minimum of this function. Usually we want to find minimum of MSE function, but there are a few loss functions, so let's practice a bit with unusual one. \n","So, to find minimum we need to find extremum of this functin. Formally it is also includes maximum, but it is not stable equlibrium point when gradient descent is performed. "]},{"metadata":{"colab_type":"text","id":"FSXKKpSHQTGb"},"cell_type":"markdown","source":["Now we need to write GD algorithm:"]},{"metadata":{"colab_type":"code","id":"SkrIgmNjQTGh","colab":{}},"cell_type":"code","source":["def grad_descent(lr, num_iter=100):\n","    \"\"\"\n","    function which performs algorithm\n","        param lr: learning rate\n","        param num_iter: number of iterations performed before algorithm terminates\n","    \"\"\"\n","    global f\n","    # some init values\n","    cur_x1, cur_x2 = 1.5, -1\n","    # here we will store arguments and value of the function which is optimized\n","    steps = []\n","    \n","    # loop iteration - gradient descent iteration\n","    for iter_num in range(num_iter):\n","        steps.append([cur_x1, cur_x2, f(cur_x1, cur_x2)])\n","        \n","        # evaluating gradient in x = (cur_x1, cur_x2)\n","        grad_x1 = #<your code is here>\n","        grad_x2 = #<your code is here>\n","                 \n","        # renew cur_x1 and cur_x2 from previouse iteration\n","        cur_x1 -= #<your code is here>\n","        cur_x2 -= #<your code is here>\n","    return np.array(steps)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ZGb1_KeoQTGo"},"cell_type":"markdown","source":["launching the algorithm:"]},{"metadata":{"colab_type":"code","id":"qtST4hWNQTGq","scrolled":true,"colab":{}},"cell_type":"code","source":["steps = grad_descent(lr=0.5, num_iter=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KwK3-hk81SPI","colab_type":"text"},"cell_type":"markdown","source":["Now it is time to visualise steps of GD algorithm. There are three axis to visualise so it will be 3D plot: $(x_1, x_2, f(x_1, x_2))$. There will be some star-points on this plots they represent the trajectory of the algorithm."]},{"metadata":{"colab_type":"text","id":"NBmNcA84QTHH"},"cell_type":"markdown","source":["If you are successful in implementation you will see how these star-points are converging to some local and, if you are lucky, to some golobal minimum. "]},{"metadata":{"colab_type":"code","id":"MjGrzSm7QTHK","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import numpy as np\n","\n","path = []\n","\n","X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","\n","ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2], marker='*', markersize=20,\n","                markerfacecolor='y', lw=3, c='black')\n","\n","ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm)\n","ax.set_zlim(0, 5)\n","ax.view_init(elev=60)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"6pHTQAyiQTHb"},"cell_type":"markdown","source":["#### Answer for Test Question №1"]},{"metadata":{"colab_type":"text","id":"Z8dDIHnfQTHd"},"cell_type":"markdown","source":["launch your grad_descent with params lr=0.3, num_iter=20 and start point (cur_x1, cur_x2) = (1.5, -1). Sum of x1, x2 and f(x1, x2) (sum of entries in steps[-1]), multiplied by ${10}^{6}$ and rounded to second dight after point is the answer."]},{"metadata":{"id":"_sLcpZ_G1SPo","colab_type":"code","colab":{}},"cell_type":"code","source":["#<your code is here>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"4TX6g1QZQTHo"},"cell_type":"markdown","source":["### 2. Linear Models."]},{"metadata":{"id":"Y4Dc0rTB1SP6","colab_type":"text"},"cell_type":"markdown","source":["We are taking code for this part from the seminar about linear model. There are two ways of solving lenear regression problem: analytical and with gradient descent. Here we are performing gradient descent algorithm"]},{"metadata":{"colab_type":"code","id":"FoCbrldkQTHr","colab":{}},"cell_type":"code","source":["W = None\n","b = None\n","\n","def mse(preds, y):\n","    return ((preds - y)**2).mean()\n","    \n","def grad_descent(X, y, lr, num_iter=100):\n","    global W, b\n","    np.random.seed(40)\n","    W = np.random.rand(X.shape[1])\n","    b = np.array(np.random.rand(1))\n","    \n","    losses = []\n","    \n","    N = X.shape[0]\n","    for iter_num in range(num_iter):\n","        preds = predict(X)\n","        losses.append(mse(preds, y))\n","        \n","        w_grad = np.zeros_like(W)\n","        b_grad = 0\n","        for sample, prediction, label in zip(X, preds, y):\n","            w_grad += 2 * (prediction - label) * sample\n","            b_grad += 2 * (prediction - label)\n","            \n","        W -= lr * w_grad\n","        b -= lr * b_grad\n","    return losses\n","\n","def predict(X):\n","    global W, b\n","    return np.squeeze(X @ W + b.reshape(-1, 1))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"RK82AgNiQTIm"},"cell_type":"markdown","source":["This is our funcion:  \n","\n","$$f(x, y) = 0.43x+0.5y + 0.67$$  "]},{"metadata":{"id":"PuE5N-aV1SQM","colab_type":"text"},"cell_type":"markdown","source":["***Friendly reminder:***  \n","The idea is to recreate our function basing on a few points from it. The problem is that we have let say 30 point to recreate the continiouse function. Seems like it is not enought, but as we found out enought for approximation."]},{"metadata":{"colab_type":"text","id":"L_xGmQytQTIp"},"cell_type":"markdown","source":["We will generate some noisy data from our fucntion:"]},{"metadata":{"colab_type":"code","id":"0huf-NLSQTIr","colab":{}},"cell_type":"code","source":["np.random.seed(40)\n","func = lambda x, y: (0.43*x + 0.5*y + 0.67 + np.random.normal(0, 7, size=x.shape))\n","\n","X = np.random.sample(size=(30)) * 10\n","Y = np.random.sample(size=(30)) * 150\n","result_train = [func(x, y) for x, y in zip(X, Y)]\n","data_train = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)\n","\n","pd.DataFrame({'x': X, 'y': Y, 'res': result_train}).head()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"P27pYzWoQTIy"},"cell_type":"markdown","source":["Let's see how do they look like:"]},{"metadata":{"colab_type":"code","id":"aNGbIB30QTIz","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","\n","ax.view_init(elev=60)\n","plt.ion()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"d5WJI3ZEQTI5"},"cell_type":"markdown","source":["Here we go. Now we are trying our GD algorithm."]},{"metadata":{"colab_type":"code","id":"kth4U5hNQTI5","colab":{}},"cell_type":"code","source":["losses = grad_descent(data_train, result_train, 1e-2, 5)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"mkzyJUfTQTJA","colab":{}},"cell_type":"code","source":["W, b"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"tDrq-Ll7QTJF"},"cell_type":"markdown","source":["Loss plot:"]},{"metadata":{"colab_type":"code","id":"8B_wmREJQTJG","colab":{}},"cell_type":"code","source":["plt.plot(losses), losses[-1];"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"COWW54UtQTJK"},"cell_type":"markdown","source":["Also our Plane:"]},{"metadata":{"colab_type":"code","id":"tQK8bWCkQTJL","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","ax.plot_surface(X,Y, W[0]*X + W[1]*Y + b, color='blue', alpha=0.3)\n","\n","ax.view_init(elev=60)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"yRJKW_6nQTJQ"},"cell_type":"markdown","source":["green one is ground truth and blue one is our prediction"]},{"metadata":{"id":"K-ZQpZQS1SSQ","colab_type":"text"},"cell_type":"markdown","source":["Seems wrong. What the matter?\n","In our model some attributes are big, some small and linear models are performing poor on such data. It is possible to solve this problem with **data normalization**.\n","There are some ways:<br>\n","\n","1) StandardScaling:<br>\n","\n","$$x_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$$  \n","(j -- attribute number, i -- point number)  \n","It means that we are pretending that all data is from some kind of normal distribution\n","\n","2) MinMaxScaling:  \n","\n","$$x_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}$$    \n","(j -- attribute number, i -- point number)  \n","It means that we are pretending that data is from [0,1] secment"]},{"metadata":{"colab_type":"text","id":"O53dA6urQTJT"},"cell_type":"markdown","source":["Using StandardScaling:"]},{"metadata":{"colab_type":"text","id":"2beSQbSxQTJV"},"cell_type":"markdown","source":["At first it is useful to look at data before normalization:"]},{"metadata":{"colab_type":"code","id":"da68g2qaQTJW","colab":{}},"cell_type":"code","source":["data_train.mean(axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Nw8o5eiMQTJZ","colab":{}},"cell_type":"code","source":["data_train.std(axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"4dHFvblpQTJd"},"cell_type":"markdown","source":["It means that in fist col values are here: 5.6 $\\pm$ 2.7, in the second one are here: 70.8 $\\pm$ 45.3. There are some difference. Your task is to perform StandaredScaling to you data."]},{"metadata":{"id":"DW1ZbTZ61STB","colab_type":"code","colab":{}},"cell_type":"code","source":["#<your code is here>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-ajY7dpHQTJq"},"cell_type":"markdown","source":["Now we are trying again to learn dependence but with normolized data"]},{"metadata":{"colab_type":"code","id":"pr3tp7sVQTJr","colab":{}},"cell_type":"code","source":["losses = grad_descent(data_train_normalized, result_train, 1e-2, 100)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"USxxGl5UQTJu","colab":{}},"cell_type":"code","source":["W, b"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Sv-1yVzFQTJx","colab":{}},"cell_type":"code","source":["plt.plot(losses)\n","print(losses[-1]);"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"PMTQKR8CQTJ0"},"cell_type":"markdown","source":["That's better"]},{"metadata":{"colab_type":"text","id":"HN3mij3XQTJ1"},"cell_type":"markdown","source":["What about our Plane:"]},{"metadata":{"colab_type":"code","id":"QAWEphcPQTJ2","colab":{}},"cell_type":"code","source":["# %matplotlib osx\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.gca(projection='3d')\n","\n","ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n","X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n","ax.plot_surface(X, Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n","X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n","ax.plot_surface(X, Y, W[0]*(X - np.mean(X)) / np.std(X) + W[1]*(Y - np.mean(Y)) / np.std(Y) + b, color='blue', alpha=0.3)\n","\n","ax.view_init(elev=60)\n","plt.ion()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"KI-3B838QTJ7"},"cell_type":"markdown","source":["#### Test Question №2"]},{"metadata":{"id":"gQncB8gk1SUE","colab_type":"text"},"cell_type":"markdown","source":["Launch GD on Normalized data wiht lr=1e-2, num_iter=200. Then add last tree loss values and round the sum to two dights after the point. This is the answer."]},{"metadata":{"colab_type":"code","id":"K_X7kB7nQTJ8","colab":{}},"cell_type":"code","source":["#<your code is here>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"uZ4k-3DuQTJ_"},"cell_type":"markdown","source":["### Regularization"]},{"metadata":{"id":"4ooNkIaY1SUN","colab_type":"text"},"cell_type":"markdown","source":["In this notebook vanilla linear regression was described. But there are augmented ones. One called Lasso, another one Ridge. They have augmented loss which is differs with vanilla one only with linear and squared penalty of the weits. "]},{"metadata":{"colab_type":"text","id":"0UOfVmxNQTKB"},"cell_type":"markdown","source":["Considering our loss as Mean Squared Error (MSE) then Ridge regression looks like this:\n","\n","$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum W_i^2$$  \n","\n","And Lasso regression:  \n","\n","$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum |W_i|$$  \n","\n","\n","Where $\\alpha$ is hyperparameter which is defined before launch"]},{"metadata":{"colab_type":"text","id":"Aan0mMHeQTKC"},"cell_type":"markdown","source":["Regularization may prevent from **overfitting**"]},{"metadata":{"colab_type":"text","id":"jxfJREvIQTKI"},"cell_type":"markdown","source":["### Useful links:\n","If you want to dive deeper in math which is behind all processes described here you may want to check this book out:\n","https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf"]}]}